{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Self-Supervised CMSF-KM tests.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyikNXbYxhq6","executionInfo":{"status":"ok","timestamp":1651055409213,"user_tz":420,"elapsed":872,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}},"outputId":"1c426180-8fa4-4577-c162-82145a6a8350"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Mon_Oct_12_20:09:46_PDT_2020\n","Cuda compilation tools, release 11.1, V11.1.105\n","Build cuda_11.1.TC455_06.29190527_0\n"]}],"source":["#GPU runtime required, should give CUDA version\n","!nvcc --version"]},{"cell_type":"code","source":["!pip install faiss-gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFauSYcBymkd","executionInfo":{"status":"ok","timestamp":1651055422453,"user_tz":420,"elapsed":12840,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}},"outputId":"2c073d74-da4b-4643-9197-67a413fd6f26"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[K     |████████████████████████████████| 85.5 MB 123 kB/s \n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SA5CzMz1tAp","executionInfo":{"status":"ok","timestamp":1651055450272,"user_tz":420,"elapsed":27840,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}},"outputId":"8c408a2a-2362-4332-fabc-7b66f91a7179"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!cp -r /content/gdrive/MyDrive/Explainable_Wound_Classification/CMSF/self_supervised/* /content"],"metadata":{"id":"lu1Z3VWPP2fk","executionInfo":{"status":"ok","timestamp":1651055458377,"user_tz":420,"elapsed":8117,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import builtins\n","import os\n","import sys\n","import time\n","import argparse\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","from torchvision import transforms, datasets\n","\n","from PIL import ImageFilter, Image\n","from util import adjust_learning_rate, AverageMeter, subset_classes\n","import models.resnet as resnet\n","from tools import get_logger\n","\n","import pdb\n","import faiss\n","\n","import numpy as np\n","import pandas as pd\n","from collections import namedtuple"],"metadata":{"id":"sbLSB0x9ypbr","executionInfo":{"status":"ok","timestamp":1651055462960,"user_tz":420,"elapsed":4588,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def get_mlp(inp_dim, hidden_dim, out_dim):\n","    mlp = nn.Sequential(\n","        nn.Linear(inp_dim, hidden_dim),\n","        nn.BatchNorm1d(hidden_dim),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(hidden_dim, out_dim),\n","    )\n","    return mlp"],"metadata":{"id":"_kesUL1R3Aey","executionInfo":{"status":"ok","timestamp":1651055462961,"user_tz":420,"elapsed":9,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def faiss_kmeans(feats, nmb_clusters):\n","    feats = feats.numpy()\n","    d = feats.shape[-1]\n","    clus = faiss.Clustering(d, nmb_clusters)\n","    clus.niter = 20\n","    clus.max_points_per_centroid = 10000000\n","\n","    index = faiss.IndexFlatL2(d)\n","    co = faiss.GpuMultipleClonerOptions()\n","    co.useFloat16 = True\n","    co.shard = True\n","    index = faiss.index_cpu_to_all_gpus(index, co)\n","\n","    # perform the training\n","    clus.train(feats, index)\n","    _, train_a = index.search(feats, 1)\n","\n","    return list(train_a[:, 0])"],"metadata":{"id":"UQ0DsdOY3ghX","executionInfo":{"status":"ok","timestamp":1651055462962,"user_tz":420,"elapsed":8,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def get_shuffle_ids(bsz):\n","    \"\"\"generate shuffle ids for ShuffleBN\"\"\"\n","    forward_inds = torch.randperm(bsz).long().cuda()\n","    backward_inds = torch.zeros(bsz).long().cuda()\n","    value = torch.arange(bsz).long().cuda()\n","    backward_inds.index_copy_(0, forward_inds, value)\n","    return forward_inds, backward_inds\n"],"metadata":{"id":"K2ZY6QzB7OrS","executionInfo":{"status":"ok","timestamp":1651055462963,"user_tz":420,"elapsed":8,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class ConstrainedMeanShiftKM(nn.Module):\n","    def __init__(self, arch, m=0.99, mem_bank_size=128000, topk=5, dataset_size=100, num_clusters=50000):\n","        super(ConstrainedMeanShiftKM, self).__init__()\n","\n","        # save parameters\n","        self.m = m\n","        self.mem_bank_size = mem_bank_size\n","        self.topk = topk\n","        self.dataset_size = dataset_size\n","        self.num_clusters = num_clusters\n","\n","        # create encoders and projection layers\n","        # both encoders should have same arch\n","        if 'resnet' in arch:\n","            self.encoder_q = resnet.__dict__[arch]()\n","            self.encoder_t = resnet.__dict__[arch]()\n","\n","        # save output embedding dimensions\n","        # assuming that both encoders have same dim\n","        feat_dim = self.encoder_q.fc.in_features\n","        hidden_dim = feat_dim * 2\n","        proj_dim = feat_dim // 4\n","\n","        # projection layers\n","        self.encoder_t.fc = get_mlp(feat_dim, hidden_dim, proj_dim)\n","        self.encoder_q.fc = get_mlp(feat_dim, hidden_dim, proj_dim)\n","\n","        # prediction layer\n","        self.predict_q = get_mlp(proj_dim, hidden_dim, proj_dim)\n","\n","        # copy query encoder weights to target encoder\n","        for param_q, param_t in zip(self.encoder_q.parameters(), self.encoder_t.parameters()):\n","            param_t.data.copy_(param_q.data)\n","            param_t.requires_grad = False\n","\n","        print(\"using mem-bank size {}\".format(self.mem_bank_size))\n","        # setup queue (For Storing Random Targets)\n","        self.register_buffer('queue', torch.randn(self.mem_bank_size, proj_dim))\n","        self.register_buffer('pool', torch.randn(self.dataset_size, proj_dim))\n","        self.register_buffer('pseudo_labels', 0*torch.ones(self.dataset_size).long())\n","        # normalize the queue embeddings\n","        self.queue = nn.functional.normalize(self.queue, dim=1)\n","        # initialize the labels queue (For Purity measurement)\n","        self.register_buffer('labels', -1*torch.ones(self.mem_bank_size).long())\n","        self.register_buffer('index_queue', -1 * torch.ones(self.mem_bank_size).long())\n","        # setup the queue pointer\n","        self.register_buffer('queue_ptr', torch.zeros(1, dtype=torch.long))\n","\n","    @torch.no_grad()\n","    def _momentum_update_target_encoder(self):\n","        for param_q, param_t in zip(self.encoder_q.parameters(), self.encoder_t.parameters()):\n","            param_t.data = param_t.data * self.m + param_q.data * (1. - self.m)\n","\n","    @torch.no_grad()\n","    def data_parallel(self):\n","        self.encoder_q = torch.nn.DataParallel(self.encoder_q)\n","        self.encoder_t = torch.nn.DataParallel(self.encoder_t)\n","        self.predict_q = torch.nn.DataParallel(self.predict_q)\n","\n","    @torch.no_grad()\n","    def cluster(self):\n","        print('start clustering ... num clusters: {}'.format(self.num_clusters))\n","        cluster_assignment = faiss_kmeans(self.pool.clone().cpu(), self.num_clusters)\n","        self.pseudo_labels = torch.tensor(cluster_assignment).cuda()\n","\n","    @torch.no_grad()\n","    def _dequeue_and_enqueue(self, targets, labels, indices):\n","        batch_size = targets.shape[0]\n","\n","        ptr = int(self.queue_ptr)\n","        assert self.mem_bank_size % batch_size == 0 \n","\n","        # replace the targets at ptr (dequeue and enqueue)\n","        self.pool[indices, :] = targets\n","        self.queue[ptr:ptr + batch_size] = targets\n","        self.labels[ptr:ptr + batch_size] = labels\n","        self.index_queue[ptr:ptr + batch_size] = indices\n","        ptr = (ptr + batch_size) % self.mem_bank_size  # move pointer\n","\n","        self.queue_ptr[0] = ptr\n","\n","    def forward(self, im_q, im_t, labels, indices):\n","        # compute query features\n","        feat_q = self.encoder_q(im_q)\n","        # compute predictions for instance level regression loss\n","        query = self.predict_q(feat_q)\n","        query = nn.functional.normalize(query, dim=1)\n","\n","        # compute target features\n","        with torch.no_grad():\n","            # update the target encoder\n","            self._momentum_update_target_encoder()\n","\n","            # shuffle targets\n","            shuffle_ids, reverse_ids = get_shuffle_ids(im_t.shape[0])\n","            im_t = im_t[shuffle_ids]\n","\n","            # forward through the target encoder\n","            current_target = self.encoder_t(im_t)\n","            current_target = nn.functional.normalize(current_target, dim=1)\n","\n","            # undo shuffle\n","            current_target = current_target[reverse_ids].detach()\n","\n","            # update the memory-bank\n","            self._dequeue_and_enqueue(current_target, labels, indices)\n","\n","        targets = self.queue.clone().detach()\n","\n","        # get pseudo of target and memory bank samples\n","        current_target_pseudo_labels = self.pseudo_labels[indices]\n","        targets_pseudo_labels = self.pseudo_labels[self.index_queue]\n","\n","        # create a mask to constrain the search space\n","        b = current_target_pseudo_labels.shape[0]\n","        m = targets_pseudo_labels.shape[0]\n","        lx = current_target_pseudo_labels.unsqueeze(1).expand((b, m))\n","        lm = targets_pseudo_labels.unsqueeze(0).expand((b, m))\n","        msk = lx != lm\n","\n","        # calculate distances between vectors\n","        dist_t = 2 - 2 * torch.einsum('bc,kc->bk', [current_target, targets])\n","        dist_q = 2 - 2 * torch.einsum('bc,kc->bk', [query, targets])\n","\n","        # select the k nearest neighbors [with smallest distance (largest=False)] based on current target\n","        _, unconstrained_nn_index = dist_t.topk(self.topk, dim=1, largest=False)\n","\n","        # select the k nearest neighbors based on constrained memory bank\n","        dist_t[torch.where(msk)] = 5.0\n","        _, constrained_nn_index = dist_t.topk(self.topk, dim=1, largest=False)\n","\n","        # calculate mean shift regression loss\n","        nn_dist_q_constrained = torch.gather(dist_q, 1, constrained_nn_index)\n","        nn_dist_q_unconstrained = torch.gather(dist_q, 1, unconstrained_nn_index)\n","\n","        # purity based on memory bank\n","        labels = labels.unsqueeze(1).expand(nn_dist_q_unconstrained.shape[0], nn_dist_q_unconstrained.shape[1])\n","        labels_queue = self.labels.clone().detach()\n","        labels_queue = labels_queue.unsqueeze(0).expand((nn_dist_q_unconstrained.shape[0], self.mem_bank_size))\n","        labels_queue = torch.gather(labels_queue, dim=1, index=unconstrained_nn_index)\n","# TODO: Change matches here\n","        matches = (labels_queue == labels).float()\n","        purity = (matches.sum(dim=1) / self.topk).mean()\n","\n","        loss = ((nn_dist_q_constrained.sum(dim=1) / self.topk).mean()\n","                + (nn_dist_q_unconstrained.sum(dim=1) / self.topk).mean()) / 2.0\n","\n","        return loss, purity"],"metadata":{"id":"9WrVPdUa3m1Q","executionInfo":{"status":"ok","timestamp":1651055463381,"user_tz":420,"elapsed":425,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class TwoCropsTransform:\n","    \"\"\"Take two random crops of one image as the query and target.\"\"\"\n","    def __init__(self, weak_transform, strong_transform):\n","        self.weak_transform = weak_transform\n","        self.strong_transform = strong_transform\n","        print(self.weak_transform)\n","        print(self.strong_transform)\n","\n","    def __call__(self, x):\n","        q = self.strong_transform(x)\n","        t = self.weak_transform(x)\n","        return [q, t]\n","\n","\n","class GaussianBlur(object):\n","    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n","\n","    def __init__(self, sigma):\n","        self.sigma = sigma\n","\n","    def __call__(self, x):\n","        sigma = random.uniform(self.sigma[0], self.sigma[1])\n","        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n","        return x"],"metadata":{"id":"8lqpv_uE7Dsp","executionInfo":{"status":"ok","timestamp":1651055463382,"user_tz":420,"elapsed":23,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class Image_Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, root_dir, label_fn, transform=None):\n","        \"\"\"\n","        Image dataset. Returns tensorized images and labels with index\n","        Args:\n","            root_dir: path to a cropped mouse image dataset.\n","            label_fn: function that returns the correct label given an image name\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.label_fn = label_fn\n","        self.transform = transform\n","\n","        samples = []\n","        for f in os.listdir(root_dir):\n","            samples.append(os.path.join(root_dir, f))\n","        \n","        self.samples = samples\n","\n","    def pil_loader(self, path):\n","        # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","        with open(path, 'rb') as f:\n","            img = Image.open(f)\n","            return img.convert('RGB')\n","\n","    def __getitem__(self, index: int):\n","            \"\"\"\n","            Returns index, tensor data, and tensorized label.\n","            \"\"\"\n","            img = self.pil_loader(self.samples[index])\n","            target = self.label_fn(self.samples[index])\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return index, img, torch.tensor(target)\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __str__(self):\n","        return \"Image_Dataset:\\n\" + \"Found \" + str(len(self)) + \" images in \" + self.root_dir + \"\\n\""],"metadata":{"id":"S7XGxXyYA87o","executionInfo":{"status":"ok","timestamp":1651055463383,"user_tz":420,"elapsed":23,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Create train loader\n","def get_train_loader(datapath, label_fn, batch_size, num_workers, weak_strong=True):\n","    traindir = os.path.join(datapath, 'train')\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","    normalize = transforms.Normalize(mean=mean, std=std)\n","\n","    augmentation_strong = [\n","        transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n","        transforms.RandomApply([\n","            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n","        ], p=0.8),\n","        transforms.RandomGrayscale(p=0.2),\n","        transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        normalize\n","    ]\n","\n","    augmentation_weak = [\n","        transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        normalize,\n","    ]\n","\n","    if weak_strong:\n","        train_dataset = Image_Dataset(\n","            traindir, label_fn, \n","            TwoCropsTransform(transforms.Compose(augmentation_weak), transforms.Compose(augmentation_strong))\n","        )\n","    else:\n","        train_dataset = Image_Dataset(\n","            traindir, label_fn, \n","            TwoCropsTransform(transforms.Compose(augmentation_strong), transforms.Compose(augmentation_strong))\n","        )\n","\n","    print('==> train dataset')\n","    print(train_dataset)\n","\n","    # NOTE: remove drop_last\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=True,\n","        num_workers=num_workers, pin_memory=True, drop_last=True)\n","\n","    return train_loader"],"metadata":{"id":"5MMumuTc7qmF","executionInfo":{"status":"ok","timestamp":1651055463383,"user_tz":420,"elapsed":22,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def train(epoch, train_loader, mean_shift, optimizer, print_freq):\n","    \"\"\"\n","    one epoch training\n","    \"\"\"\n","    mean_shift.train()\n","\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    loss_meter = AverageMeter()\n","    purity_meter = AverageMeter()\n","\n","    end = time.time()\n","    for idx, (indices, (im_q, im_t), labels) in enumerate(train_loader):\n","        data_time.update(time.time() - end)\n","        im_q = im_q.cuda(non_blocking=True)\n","        im_t = im_t.cuda(non_blocking=True)\n","        labels = labels.cuda(non_blocking=True)\n","\n","        # ===================forward=====================\n","        loss, purity = mean_shift(im_q=im_q, im_t=im_t, labels=labels, indices=indices)\n","\n","        # ===================backward=====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # ===================meters=====================\n","        loss_meter.update(loss.item(), im_q.size(0))\n","        purity_meter.update(purity.item(), im_q.size(0))\n","\n","        torch.cuda.synchronize()\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        # print info\n","        if (idx + 1) % print_freq == 0:\n","            print('Train: [{0}][{1}/{2}]\\t'\n","                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'purity {purity.val:.3f} ({purity.avg:.3f})\\t'\n","                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'.format(\n","                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n","                   data_time=data_time,\n","                   purity=purity_meter,\n","                   loss=loss_meter))\n","            sys.stdout.flush()\n","            sys.stdout.flush()\n","\n","    return loss_meter.avg"],"metadata":{"id":"kbeHAYjbVSaV","executionInfo":{"status":"ok","timestamp":1651055463384,"user_tz":420,"elapsed":22,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#workaround struct to pass args to util script\n","LRArgs = namedtuple('LRArgs', ['cos', 'learning_rate', 'epochs', 'lr_decay_rate'])"],"metadata":{"id":"rXCJ1Q94dBEZ","executionInfo":{"status":"ok","timestamp":1651055463385,"user_tz":420,"elapsed":23,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def cmsf_km_main(data_path, checkpoint_path, label_fn, batch_size=16, num_workers=2, \n","                 epochs=200, print_freq=10, save_freq=10, weak_strong=True, \n","                 debug=False, arch='resnet50', momentum=0.99, mem_bank_size=128000, \n","                 topk=5, num_clusters=4, learning_rate=0.05, sgd_momentum=0.9, \n","                 weight_decay=1e-4, weights=None, resume=None, cos=True, \n","                 lr_decay_rate=0.2):\n","    \"\"\"\n","    todo: docstring\n","    \"\"\"\n","    opt = locals()\n","    del opt['label_fn']\n","\n","    os.makedirs(checkpoint_path, exist_ok=True)\n","\n","    train_loader = get_train_loader(data_path, label_fn, batch_size, num_workers, weak_strong)\n","\n","    mean_shift = ConstrainedMeanShiftKM(\n","        arch,\n","        m=momentum,\n","        mem_bank_size=mem_bank_size,\n","        topk=topk,\n","        dataset_size=len(train_loader.dataset),\n","        num_clusters=num_clusters\n","    )\n","\n","    mean_shift.data_parallel()\n","    mean_shift = mean_shift.cuda()\n","    print(mean_shift)\n","\n","    params = [p for p in mean_shift.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params,\n","                                lr=learning_rate,\n","                                momentum=sgd_momentum,\n","                                weight_decay=weight_decay)\n","\n","    cudnn.benchmark = True\n","    start_epoch = 1\n","\n","    if weights:\n","        print('==> load weights from checkpoint: {}'.format(weights))\n","        ckpt = torch.load(weights)\n","        print('==> resume from epoch: {}'.format(ckpt['epoch']))\n","        if 'model' in ckpt:\n","            sd = ckpt['model']\n","        else:\n","            sd = ckpt['state_dict']\n","        msg = mean_shift.load_state_dict(sd, strict=False)\n","        optimizer.load_state_dict(ckpt['optimizer'])\n","        start_epoch = ckpt['epoch'] + 1\n","        print(msg)\n","\n","    if resume:\n","        print('==> resume from checkpoint: {}'.format(resume))\n","        ckpt = torch.load(resume, map_location='cpu')\n","        # sd = ckpt['state_dict']\n","        # sd = {k.replace('module.', ''): v for k, v in sd.items()}\n","        print('==> resume from epoch: {}'.format(ckpt['epoch']))\n","        mean_shift.load_state_dict(ckpt['state_dict'], strict=True)\n","        optimizer.load_state_dict(ckpt['optimizer'])\n","        start_epoch = ckpt['epoch'] + 1\n","\n","    lr_args = LRArgs(cos, learning_rate, epochs, lr_decay_rate)\n","\n","    for epoch in range(start_epoch, epochs + 1):\n","\n","        adjust_learning_rate(epoch, lr_args, optimizer)\n","        print(\"==> training...\")\n","\n","        time1 = time.time()\n","\n","        train(epoch, train_loader, mean_shift, optimizer, print_freq)\n","        mean_shift.cluster()\n","        time2 = time.time()\n","        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n","\n","        # saving the model\n","        if epoch % save_freq == 0:\n","            print('==> Saving...')\n","            state = {\n","                'opt': opt,\n","                'state_dict': mean_shift.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'epoch': epoch,\n","            }\n","\n","            save_file = os.path.join(checkpoint_path, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n","            torch.save(state, save_file)\n","\n","            # help release GPU memory\n","            del state\n","            torch.cuda.empty_cache()"],"metadata":{"id":"nbS9piVdV8T_","executionInfo":{"status":"ok","timestamp":1651055818868,"user_tz":420,"elapsed":893,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["root_path = 'gdrive/MyDrive/Explainable_Wound_Classification/'"],"metadata":{"id":"4vsk9a5eihry","executionInfo":{"status":"ok","timestamp":1651055820845,"user_tz":420,"elapsed":3,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["cmsf_km_main(\n","    data_path=root_path + 'Split_images', \n","    checkpoint_path=root_path + 'outputs/test1', \n","    label_fn=lambda x: 0,\n","    batch_size=16,\n","    num_workers=2,\n","    epochs=20,\n","    arch='resnet50',\n","    topk=5,\n","    num_clusters=4,\n","    learning_rate=0.05,\n","    mem_bank_size=128000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"udb2Oop0jJ7y","executionInfo":{"status":"ok","timestamp":1651055970877,"user_tz":420,"elapsed":147565,"user":{"displayName":"Theophanis Fox","userId":"02229710404136063587"}},"outputId":"a7335869-cfe0-42f7-b660-a9eae68c44f0"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Compose(\n","    RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n","    RandomHorizontalFlip(p=0.5)\n","    ToTensor()\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",")\n","Compose(\n","    RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n","    RandomApply(\n","    p=0.8\n","    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])\n",")\n","    RandomGrayscale(p=0.2)\n","    RandomApply(\n","    p=0.5\n","    <__main__.GaussianBlur object at 0x7fb4d5692690>\n",")\n","    RandomHorizontalFlip(p=0.5)\n","    ToTensor()\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",")\n","==> train dataset\n","Image_Dataset:\n","Found 191 images in gdrive/MyDrive/Explainable_Wound_Classification/Split_images/train\n","\n","using mem-bank size 128000\n","ConstrainedMeanShiftKM(\n","  (encoder_q): DataParallel(\n","    (module): ResNet(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n","      (fc): Sequential(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Linear(in_features=4096, out_features=512, bias=True)\n","      )\n","    )\n","  )\n","  (encoder_t): DataParallel(\n","    (module): ResNet(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n","      (fc): Sequential(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Linear(in_features=4096, out_features=512, bias=True)\n","      )\n","    )\n","  )\n","  (predict_q): DataParallel(\n","    (module): Sequential(\n","      (0): Linear(in_features=512, out_features=4096, bias=True)\n","      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Linear(in_features=4096, out_features=512, bias=True)\n","    )\n","  )\n",")\n","LR: 0.05\n","==> training...\n","Train: [1][10/11]\tBT 0.546 (0.621)\tDT 0.000 (0.064)\tpurity 1.000 (1.000)\tloss 0.804 (0.944)\t\n","start clustering ... num clusters: 4\n","epoch 1, total time 7.14\n","LR: 0.04969220851487845\n","==> training...\n","Train: [2][10/11]\tBT 0.552 (0.604)\tDT 0.000 (0.050)\tpurity 1.000 (1.000)\tloss 0.661 (0.674)\t\n","start clustering ... num clusters: 4\n","epoch 2, total time 6.98\n","LR: 0.048776412907378844\n","==> training...\n","Train: [3][10/11]\tBT 0.540 (0.600)\tDT 0.000 (0.053)\tpurity 1.000 (1.000)\tloss 0.674 (0.690)\t\n","start clustering ... num clusters: 4\n","epoch 3, total time 6.94\n","LR: 0.047275163104709195\n","==> training...\n","Train: [4][10/11]\tBT 0.547 (0.602)\tDT 0.000 (0.049)\tpurity 1.000 (1.000)\tloss 0.921 (0.716)\t\n","start clustering ... num clusters: 4\n","epoch 4, total time 6.96\n","LR: 0.04522542485937369\n","==> training...\n","Train: [5][10/11]\tBT 0.552 (0.600)\tDT 0.000 (0.048)\tpurity 1.000 (1.000)\tloss 0.492 (0.626)\t\n","start clustering ... num clusters: 4\n","epoch 5, total time 6.93\n","LR: 0.04267766952966369\n","==> training...\n","Train: [6][10/11]\tBT 0.550 (0.605)\tDT 0.000 (0.051)\tpurity 1.000 (1.000)\tloss 0.768 (0.578)\t\n","start clustering ... num clusters: 4\n","epoch 6, total time 6.99\n","LR: 0.03969463130731183\n","==> training...\n","Train: [7][10/11]\tBT 0.541 (0.599)\tDT 0.000 (0.052)\tpurity 1.000 (1.000)\tloss 0.527 (0.497)\t\n","start clustering ... num clusters: 4\n","epoch 7, total time 6.94\n","LR: 0.03634976249348867\n","==> training...\n","Train: [8][10/11]\tBT 0.550 (0.600)\tDT 0.000 (0.050)\tpurity 1.000 (1.000)\tloss 0.521 (0.477)\t\n","start clustering ... num clusters: 4\n","epoch 8, total time 6.93\n","LR: 0.032725424859373686\n","==> training...\n","Train: [9][10/11]\tBT 0.548 (0.598)\tDT 0.000 (0.049)\tpurity 1.000 (1.000)\tloss 0.421 (0.432)\t\n","start clustering ... num clusters: 4\n","epoch 9, total time 6.92\n","LR: 0.028910861626005774\n","==> training...\n","Train: [10][10/11]\tBT 0.553 (0.601)\tDT 0.000 (0.051)\tpurity 1.000 (1.000)\tloss 0.525 (0.411)\t\n","start clustering ... num clusters: 4\n","epoch 10, total time 6.95\n","==> Saving...\n","LR: 0.025\n","==> training...\n","Train: [11][10/11]\tBT 0.542 (0.606)\tDT 0.000 (0.053)\tpurity 1.000 (1.000)\tloss 0.429 (0.391)\t\n","start clustering ... num clusters: 4\n","epoch 11, total time 7.00\n","LR: 0.021089138373994235\n","==> training...\n","Train: [12][10/11]\tBT 0.546 (0.610)\tDT 0.000 (0.058)\tpurity 1.000 (1.000)\tloss 0.398 (0.455)\t\n","start clustering ... num clusters: 4\n","epoch 12, total time 7.07\n","LR: 0.017274575140626316\n","==> training...\n","Train: [13][10/11]\tBT 0.554 (0.608)\tDT 0.000 (0.057)\tpurity 1.000 (1.000)\tloss 0.274 (0.361)\t\n","start clustering ... num clusters: 4\n","epoch 13, total time 7.04\n","LR: 0.013650237506511332\n","==> training...\n","Train: [14][10/11]\tBT 0.552 (0.609)\tDT 0.000 (0.055)\tpurity 1.000 (1.000)\tloss 0.376 (0.387)\t\n","start clustering ... num clusters: 4\n","epoch 14, total time 7.03\n","LR: 0.010305368692688175\n","==> training...\n","Train: [15][10/11]\tBT 0.544 (0.594)\tDT 0.000 (0.046)\tpurity 1.000 (1.000)\tloss 0.323 (0.345)\t\n","start clustering ... num clusters: 4\n","epoch 15, total time 6.88\n","LR: 0.0073223304703363135\n","==> training...\n","Train: [16][10/11]\tBT 0.544 (0.594)\tDT 0.000 (0.045)\tpurity 1.000 (1.000)\tloss 0.342 (0.417)\t\n","start clustering ... num clusters: 4\n","epoch 16, total time 6.88\n","LR: 0.004774575140626317\n","==> training...\n","Train: [17][10/11]\tBT 0.549 (0.598)\tDT 0.000 (0.052)\tpurity 1.000 (1.000)\tloss 0.623 (0.408)\t\n","start clustering ... num clusters: 4\n","epoch 17, total time 6.90\n","LR: 0.0027248368952908055\n","==> training...\n","Train: [18][10/11]\tBT 0.553 (0.597)\tDT 0.000 (0.047)\tpurity 1.000 (1.000)\tloss 0.394 (0.374)\t\n","start clustering ... num clusters: 4\n","epoch 18, total time 6.90\n","LR: 0.0012235870926211618\n","==> training...\n","Train: [19][10/11]\tBT 0.540 (0.592)\tDT 0.000 (0.046)\tpurity 1.000 (1.000)\tloss 0.614 (0.410)\t\n","start clustering ... num clusters: 4\n","epoch 19, total time 6.86\n","LR: 0.00030779148512155856\n","==> training...\n","Train: [20][10/11]\tBT 0.550 (0.600)\tDT 0.000 (0.049)\tpurity 1.000 (1.000)\tloss 0.361 (0.432)\t\n","start clustering ... num clusters: 4\n","epoch 20, total time 6.94\n","==> Saving...\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"xE1J5kPylhm4"},"execution_count":null,"outputs":[]}]}